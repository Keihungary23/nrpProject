import fitz  # PyMuPDF
from transformers import pipeline
from keybert import KeyBERT

# ================================
# 1. PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# ================================
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# ================================
# 2. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å¯¾å¿œï¼‰
# ================================
def chunk_text(text, chunk_size=1500, overlap=200):
    chunks = []
    step = chunk_size - overlap
    for i in range(0, len(text), step):
        chunks.append(text[i:i + chunk_size])
        if i + chunk_size >= len(text):
            break
    return chunks

# ================================
# 3. è¦ç´„å‡¦ç†
# ================================
def summarize_chunks(chunks, summarizer):
    summaries = []
    for i, chunk in enumerate(chunks):
        try:
            summary = summarizer(
                chunk,
                max_length=200,
                min_length=40,
                do_sample=False
            )[0]["summary_text"]
            summaries.append(summary)
            print(f"âœ… Chunk {i+1}/{len(chunks)} summarized.")
        except Exception as e:
            print(f"âš ï¸ Chunk {i+1} failed: {e}")
    return summaries

# ================================
# 4. ä¸­é–“ã¾ã¨ã‚ï¼ˆç« ãƒ¬ãƒ™ãƒ«ï¼‰
# ================================
def summarize_sections(summaries, summarizer, chunk_size=1000):
    text = " ".join(summaries)
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    section_summaries = []
    for i, chunk in enumerate(chunks):
        summary = summarizer(
            chunk,
            max_length=300,
            min_length=60,
            do_sample=False
        )[0]["summary_text"]
        section_summaries.append(summary)
        print(f"ğŸ“Œ Section {i+1}/{len(chunks)} summarized.")
    return section_summaries

# ================================
# 5. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆn-gram & MMRå¯¾å¿œï¼‰
# ================================
def extract_keywords(section_summaries, top_n=5):
    kw_model = KeyBERT()
    keywords_per_section = []
    for i, summary in enumerate(section_summaries):
        keywords = kw_model.extract_keywords(
            summary,
            top_n=top_n,
            use_mmr=True,                   # MMRæœ‰åŠ¹åŒ– â†’ å¤šæ§˜æ€§ç¢ºä¿
            diversity=0.7,                  # é¡ä¼¼èªã‚’é¿ã‘ã‚‹åº¦åˆã„
            keyphrase_ngram_range=(1, 3)    # 1ï½3èªã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’è¨±å¯
        )
        keywords_per_section.append(keywords)
        print(f"ğŸ”‘ Keywords extracted for Section {i+1}")
    return keywords_per_section


# ================================
# 6. æœ€çµ‚ã¾ã¨ã‚ï¼ˆå…¨ä½“ï¼‰
# ================================
def summarize_final(section_summaries, summarizer):
    final_input = " ".join(section_summaries)
    final_summary = summarizer(
        final_input,
        max_length=600,
        min_length=250,
        do_sample=False
    )[0]["summary_text"]
    return final_summary

# ================================
# 7. ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ================================
if __name__ == "__main__":
    pdf_path = "1706.03762v7.pdf"   # Transformer è«–æ–‡

    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    text = extract_text_from_pdf(pdf_path)
    print(f"ğŸ“„ Extracted text length: {len(text)} characters")

    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    chunks = chunk_text(text, chunk_size=1500)
    print(f"ğŸ” Split into {len(chunks)} chunks")

    # Hugging Face è¦ç´„ãƒ¢ãƒ‡ãƒ«
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

    # Step1: ãƒãƒ£ãƒ³ã‚¯è¦ç´„
    chunk_summaries = summarize_chunks(chunks, summarizer)

    # Step2: ä¸­é–“ã¾ã¨ã‚ï¼ˆç« ãƒ¬ãƒ™ãƒ«ï¼‰
    section_summaries = summarize_sections(chunk_summaries, summarizer)

    # Step3: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords_per_section = extract_keywords(section_summaries, top_n=5)

    # Step4: æœ€çµ‚ã¾ã¨ã‚
    final_summary = summarize_final(section_summaries, summarizer)

    print("\n=== Section Summaries & Keywords ===\n")
    for i, (sec, keywords) in enumerate(zip(section_summaries, keywords_per_section), 1):
        print(f"[Section {i}] {sec}\n")
        print("   ğŸ”‘ Keywords:", [kw for kw, score in keywords], "\n")

    print("\n=== Final Summary of the Paper ===\n")
    print(final_summary)
