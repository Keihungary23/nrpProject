ã„ã„é¸æŠã§ã™ï¼âœ¨
ã€Œç ”ç©¶è«–æ–‡ã‚µãƒãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆè¦ç´„ï¼‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰ã€ã¯å®Ÿç”¨æ€§ã‚‚é«˜ã„ã—ã€é™¢è©¦ï¼‹AIç³»ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒ ä¸¡æ–¹ã®ã‚¢ãƒ”ãƒ¼ãƒ«ã«åŠ¹ãã¾ã™ã€‚

ã§ã¯ã€æ•°æ™‚é–“ã§å®Œæˆã§ãã‚‹ **æœ€å°æ§‹æˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­è¨ˆå›³** ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

---

## ğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

### 1. è«–æ–‡PDFã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º

* ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¾‹:

  * `PyPDF2` / `pdfplumber` â†’ PDFã‹ã‚‰æœ¬æ–‡æŠ½å‡º
  * ï¼ˆarXivè«–æ–‡ãªã‚‰LaTeXç”±æ¥ã§æ¯”è¼ƒçš„ãã‚Œã„ã«å–ã‚Œã‚‹ï¼‰

### 2. è¦ç´„

* Hugging Faceã®è¦ç´„ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨

  * è»½é‡ãªã‚‰ `sshleifer/distilbart-cnn-12-6`
  * æ—¥æœ¬èªè«–æ–‡ãªã‚‰ `sonoisa/t5-base-japanese` ç³»åˆ—
* å‡ºåŠ›ã‚’ã€Œ3è¡Œä»¥å†…ã€ã«åˆ¶å¾¡

### 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º

* `keybert` ãŒæœ€ã‚‚ç°¡å˜

  * BERTãƒ™ãƒ¼ã‚¹ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å€™è£œã‚’æŠ½å‡º
* æ—¥æœ¬èªãªã‚‰ `keybert` + `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`

### 4. UIï¼ˆä»»æ„ï¼‰

* CLIï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ï¼‰ã§ååˆ†ã ã‘ã©ã€è¦‹ã›ã‚‹ãªã‚‰

  * `Streamlit` ã‚’ä½¿ãˆã°PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ è¦ç´„ï¼‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤ºãŒã™ãä½œã‚Œã‚‹

---

## â±ï¸ æ™‚é–“é…åˆ†ï¼ˆç›®å®‰ï¼‰

1. PDFèª­ã¿è¾¼ã¿ï¼ˆ30åˆ†ï¼‰
2. è¦ç´„ãƒ‘ãƒ¼ãƒˆï¼ˆ1æ™‚é–“ï¼‰
3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆ30åˆ†ï¼‰
4. UI or ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯æ•´å‚™ï¼ˆ1æ™‚é–“ï¼‰

åˆè¨ˆ **2ã€œ3æ™‚é–“** ã§å‹•ãã‚‚ã®ãŒä½œã‚Œã¾ã™ã€‚

---

## ğŸ’» ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆéª¨çµ„ã¿ï¼‰

```python
# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
# pip install transformers keybert pdfplumber sentence-transformers

import pdfplumber
from transformers import pipeline
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

# 1. PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# 2. è¦ç´„ãƒ¢ãƒ‡ãƒ«ï¼ˆè‹±èªä¾‹: distilbartï¼‰
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

def summarize_text(text, max_len=130, min_len=30):
    summary = summarizer(text, max_length=max_len, min_length=min_len, do_sample=False)
    return summary[0]['summary_text']

# 3. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
kw_model = KeyBERT(model=SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2'))

def extract_keywords(text, top_n=5):
    return kw_model.extract_keywords(text, top_n=top_n)

# å®Ÿè¡Œä¾‹
pdf_path = "sample_paper.pdf"
text = extract_text_from_pdf(pdf_path)

summary = summarize_text(text)
keywords = extract_keywords(text)

print("=== è¦ç´„ ===")
print(summary)
print("\n=== ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ===")
print(keywords)
```

---

## ğŸ“Œ ã‚¢ãƒ”ãƒ¼ãƒ«ã®ä»•æ–¹

* ã€Œç ”ç©¶è«–æ–‡ã®æƒ…å ±æ•´ç†ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã€
* ã€Œè‡ªç„¶è¨€èªå‡¦ç†ï¼ˆè¦ç´„ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼‰ï¼‹PDFå‡¦ç†ï¼‹UIå®Ÿè£…ã‚’çµŒé¨“ã€
* GitHubã«è¼‰ã›ã‚‹ã¨ãã¯ README ã«ä»¥ä¸‹ã‚’è¼‰ã›ã‚‹ã¨å¼·ã„ï¼š

  * ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›®çš„
  * ä½¿ã„æ–¹ï¼ˆã‚¹ã‚¯ã‚·ãƒ§ï¼‰
  * ä½¿ç”¨æŠ€è¡“ï¼ˆNLPãƒ¢ãƒ‡ãƒ«ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªåï¼‰

---

ğŸ‘‰ ã“ã“ã§è³ªå•ã§ã™ãŒã€

* è«–æ–‡ã¯ **è‹±èªä¸­å¿ƒ** ã§ã™ã‹ï¼Ÿãã‚Œã¨ã‚‚ **æ—¥æœ¬èªè«–æ–‡** ãŒå¤šãã†ã§ã™ã‹ï¼Ÿ
  ï¼ˆä½¿ã†ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã®ã§ã€ãã“ã«åˆã‚ã›ãŸå…·ä½“ä¾‹ã‚’å‡ºã›ã¾ã™ï¼ï¼‰
