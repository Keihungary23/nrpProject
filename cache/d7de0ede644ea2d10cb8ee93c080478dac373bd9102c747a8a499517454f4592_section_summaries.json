[
  " We propose the Transformer, a model architecture eschewing recurrence and relying entirely on an attention mechanism to draw global dependencies between input and output . The Transformer allows for significantly more parallelization and can reach a new state of the art in terms of language modeling and machine translation . The paper is the latest in a series of papers published on the open-source Neural-Networking platform ."
]